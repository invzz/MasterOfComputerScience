{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordTokenizer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mgSjLnVbroy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35781bcf-2e78-47cc-ce3d-bfddb689243a"
      },
      "source": [
        "\n",
        "# To execute the file: execfile(r'./wordToken.py')\n",
        "\n",
        "# (install cmudict and download NLTK only once by de-commenting - only once... - the lines below) \n",
        "#!pip install cmudict\n",
        "#nltk.download()\n",
        "#nltk.download('punkt')\n",
        "\n",
        "import nltk\n",
        "import cmudict\n",
        "\n",
        "#   MaxMatch implementation\n",
        "# code example: simple version of maxmatch algorithm for tokenization (word segmentation)\n",
        "def maxmatchtokenize(str, dict):\n",
        "    s = 0\n",
        "    words = []\n",
        "    \n",
        "    while (s < len(str)):\n",
        "        found = False\n",
        "        \n",
        "        # find biggest word in dict that matches str[s:xxx]\n",
        "        for word in dict:\n",
        "            lw = len(word)\n",
        "            if (str[s:s+lw] == word):\n",
        "                words.append(word)\n",
        "                s += lw\n",
        "                found = True\n",
        "                break\n",
        "        if (not found):\n",
        "            words.append(str[s])\n",
        "            s += 1\n",
        "\n",
        "    print(words)\n",
        "    #return words\n",
        "\n",
        "# dict = []\n",
        "# small dictionary of known words, longest words first\n",
        "dict = [\"distance\", \"before\", \"ahead\", \"canon\", \"table\", \"theta\", \"after\", \"where\", \n",
        "        \"there\", \"short\", \"bled\", \"said\", \"lead\", \"only\", \"ash\", \"can\", \"man\", \"her\", \n",
        "        \"own\", \"the\", \"ran\", \"see\", \"it\", \"we\", \"a\"]\n",
        "\n",
        "# CMU dictionary \n",
        "# dict = cmudict.words()\n",
        " \n",
        "# this algorithm is designed to work with languages that don't have whitespace characters\n",
        "# so simulate that in our test\n",
        "maxmatchtokenize(\"themanranafterit\", dict)      # works!\n",
        "maxmatchtokenize(\"thetabledownthere\", dict)     # fails!\n",
        "maxmatchtokenize(\"wecanonlyseeashortdistanceahead\", dict)     # fails!\n",
        "\n",
        "\n",
        "# word_tokenize(s) from NLTK: Tokenize a string to split off punctuation other than periods\n",
        "\n",
        "print(nltk.word_tokenize(\"themanranafterit.\"))\n",
        "print(nltk.word_tokenize(\"the man, he ran after it's $3.23 dog on 03/23/2016.\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'man', 'ran', 'after', 'it']\n",
            "['theta', 'bled', 'own', 'there']\n",
            "['we', 'canon', 'l', 'y', 'see', 'ash', 'o', 'r', 't', 'distance', 'ahead']\n",
            "['themanranafterit', '.']\n",
            "['the', 'man', ',', 'he', 'ran', 'after', 'it', \"'s\", '$', '3.23', 'dog', 'on', '03/23/2016', '.']\n"
          ]
        }
      ]
    }
  ]
}